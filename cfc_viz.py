# -*- coding: utf-8 -*-
"""CFC VIZ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AI_mGX-JGHlCg6Kckb0ciyutdAXDcAWE
"""

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns

gps_df = pd.read_csv("https://raw.githubusercontent.com/Chelsea-Fc-Performance-Insights/Competition/refs/heads/main/DATA/CFC%20GPS%20Data.csv", encoding='latin-1') # Try 'latin-1' encoding
physical_df = pd.read_csv("https://raw.githubusercontent.com/Chelsea-Fc-Performance-Insights/Competition/refs/heads/main/DATA/CFC%20Physical%20Capability%20Data_.csv", encoding='latin-1') # Try 'latin-1' encoding
recovery_df = pd.read_csv("https://raw.githubusercontent.com/Chelsea-Fc-Performance-Insights/Competition/refs/heads/main/DATA/CFC%20Recovery%20status%20Data.csv", encoding='latin-1') # Try 'latin-1' encoding
priority_df = pd.read_csv("https://raw.githubusercontent.com/Chelsea-Fc-Performance-Insights/Competition/refs/heads/main/DATA/CFC%20Individual%20Priority%20Areas.csv", encoding='latin-1') # Try 'latin-1' encoding

def auto_convert_types(df, error_threshold=0.5):
    error_count = 0

    for col in df.columns:
        # First, try converting to numeric
        try:
            df[col] = pd.to_numeric(df[col], errors='raise')  # Convert if purely numeric
            continue  # Skip to next column if conversion is successful
        except ValueError:
            error_count += 1

        # If column is still an object, check if it's categorical (low unique values)
        unique_ratio = df[col].nunique() / len(df[col])
        if unique_ratio < 0.5:  # Adjust threshold as needed
            df[col] = df[col].astype('category')

    # Proceed only if error_count exceeds the threshold
    if error_count/len(df) > error_threshold:
        return df
    else:
        print(f"Conversion not performed. Error count ({error_count/len(df)}) did not exceed the threshold ({error_threshold}).")
        return df

gps_df.replace("#NULL!", pd.NA, inplace=True)
gps_df = auto_convert_types(gps_df)

physical_df.replace("#NULL!", pd.NA, inplace=True)
physical_df = auto_convert_types(physical_df)

recovery_df.replace("#NULL!", pd.NA, inplace=True)
recovery_df = auto_convert_types(recovery_df)

priority_df.replace("#NULL!", pd.NA, inplace=True)
priority_df = auto_convert_types(priority_df)

def convert_hr_to_seconds(df, columns_to_plot):
    for col in columns_to_plot:
        if col in df.columns and 'hr' in col.lower() and '_hms' in col.lower(): #check for hr in the column name
            try:
                # Split the 'HH:MM:SS' string into hours, minutes, and seconds
                df[col] = pd.to_datetime(df[col], format='%H:%M:%S', errors='coerce').dt.time
                df[col] = df[col].astype(str)
                time_parts = df[col].str.split(':', expand=True)
                hours = pd.to_numeric(time_parts[0], errors='coerce')
                minutes = pd.to_numeric(time_parts[1], errors='coerce')
                seconds = pd.to_numeric(time_parts[2], errors='coerce')

                # Calculate the total seconds
                df[col] = hours * 3600 + minutes * 60 + seconds
            except (ValueError, AttributeError) as e:
              print(f"Error converting column '{col}': {e}") #Print error message and continue
    return df

# Example usage
colo = ['hr_zone_1_hms', 'hr_zone_2_hms', 'hr_zone_3_hms', 'hr_zone_4_hms', 'hr_zone_5_hms']
gps_df = convert_hr_to_seconds(gps_df, colo)
gps_df["day_duration"] = gps_df["day_duration"] * 60

columns_to_plot = ['hr_zone_1_hms', 'hr_zone_2_hms', 'hr_zone_3_hms', 'hr_zone_4_hms', 'hr_zone_5_hms', "day_duration"]
gps_df[columns_to_plot].head()

load_metrics = ['distance', 'distance_over_21', 'distance_over_24', 'distance_over_27', 'accel_decel_over_2_5', 'accel_decel_over_3_5', 'accel_decel_over_4_5']
std_devs = gps_df[load_metrics].std()
weights = 1 / std_devs ** 2
print(weights)

alpha, beta, gamma = weights['distance_over_21'], weights['distance_over_24'], weights['distance_over_27']
delta, epsilon, zeta = weights['accel_decel_over_2_5'], weights['accel_decel_over_3_5'], weights['accel_decel_over_4_5']

gps_df['weighted_distance'] = gps_df['distance'] + alpha * gps_df['distance_over_21'] + beta * gps_df['distance_over_24'] + gamma * gps_df['distance_over_27']
gps_df['accel_score'] = delta * gps_df['accel_decel_over_2_5'] + epsilon * gps_df['accel_decel_over_3_5'] + zeta * gps_df['accel_decel_over_4_5']

def set_xticks_smartly(ax, data, num_ticks=6, rotation=0):
    """Dynamically adjusts x-ticks to improve readability."""
    if pd.api.types.is_numeric_dtype(data):
        numeric_data = data.dropna()
        if not numeric_data.empty:
            ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=num_ticks))  # Auto-adjust tick count
            ax.tick_params(axis='x', rotation=rotation)  # Rotate labels for readability

# Assuming 'gps_df' contains the required data
columns_to_plot = ['hr_zone_1_hms', 'hr_zone_2_hms', 'hr_zone_3_hms',
                   'hr_zone_4_hms', 'hr_zone_5_hms', 'day_duration']

fig, axes = plt.subplots(2, 3, figsize=(12, 6))  # Wider layout for better spacing
axes = axes.flatten()  # Flatten for easy iteration

for i, column in enumerate(columns_to_plot):
    axes[i].hist(gps_df[column].dropna(), bins=7)  # Clean NaNs before plotting
    axes[i].set_title(f'Histogram of {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frequency')
    set_xticks_smartly(axes[i], gps_df[column], 8)

plt.tight_layout()
plt.show()

std_devs = gps_df[columns_to_plot].std()
weights = 1 / std_devs ** 2
print(weights)

gps_df['hr_intensity'] = (
    (gps_df['hr_zone_1_hms'] * weights['hr_zone_1_hms']) +
    (gps_df['hr_zone_2_hms'] * weights['hr_zone_2_hms']) +
    (gps_df['hr_zone_3_hms'] * weights['hr_zone_3_hms']) +
    (gps_df['hr_zone_4_hms'] * weights['hr_zone_4_hms']) +
    (gps_df['hr_zone_5_hms'] * weights['hr_zone_5_hms'])
) / gps_df['day_duration']

gps_df.replace(-np.inf, np.nan, inplace=True)
gps_df.fillna(gps_df.min(numeric_only=True), inplace=True)
gps_df.replace(np.inf, np.nan, inplace=True)
gps_df.fillna(gps_df.max(numeric_only=True), inplace=True)

gps_df['load_demand_score'] = (gps_df['weighted_distance'] + gps_df['accel_score']) * gps_df['hr_intensity']
gps_df[['load_demand_score','weighted_distance','accel_score','hr_intensity']].head()

# prompt: plot the distribution of 'load_demand_score','weighted_distance','accel_score','hr_intensity' using ax plot

import matplotlib.pyplot as plt
import numpy as np  # Import numpy for nan_to_num

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# Replace NaN values with a finite number before plotting
axes[0, 0].hist(np.nan_to_num(gps_df['load_demand_score']), bins=20)
axes[0, 0].set_title('Load Demand Score Distribution')

axes[0, 1].hist(np.nan_to_num(gps_df['weighted_distance']), bins=20)
axes[0, 1].set_title('Weighted Distance Distribution')

axes[1, 0].hist(np.nan_to_num(gps_df['accel_score']), bins=20)
axes[1, 0].set_title('Accel Score Distribution')

axes[1, 1].hist(np.nan_to_num(gps_df['hr_intensity']), bins=20)
axes[1, 1].set_title('HR Intensity Distribution')

plt.tight_layout()
plt.show()

gps_df[['load_demand_score','weighted_distance','accel_score','hr_intensity']].head()

gps_df['date'] = pd.to_datetime(gps_df['date'], format='%d/%m/%Y').dt.date
daily_load_demand = gps_df.groupby('date')['load_demand_score'].mean().reset_index()
daily_load_demand['acute_load'] = daily_load_demand["load_demand_score"].rolling(window=7, min_periods=1).mean()
daily_load_demand['chronic_load'] = daily_load_demand["load_demand_score"].rolling(window=28, min_periods=1).mean()
daily_load_demand['acwr'] = daily_load_demand['acute_load'] / daily_load_demand['chronic_load']

fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)  # Share x-axis for better alignment

axes[0].plot(daily_load_demand['date'], daily_load_demand["load_demand_score"], label="Load Demand", color='blue')
axes[0].set_ylabel('Load Demand Score')
axes[0].set_title('Load Demand Over Time')
axes[0].legend()
axes[0].grid(True)

axes[1].plot(daily_load_demand['date'], daily_load_demand['acute_load'], label="Acute Load", color='orange')
axes[1].plot(daily_load_demand['date'], daily_load_demand['chronic_load'], label="Chronic Load", color='green')
axes[1].set_ylabel('Load')
axes[1].set_title('Acute Load (7 Days) vs. Chronic Load (28 Days)')
axes[1].legend()
axes[1].grid(True)

axes[2].plot(daily_load_demand['date'], daily_load_demand['acwr'], label="ACWR", color='purple')
axes[2].axhline(y=1.5, color='red', linestyle='--', label='Risk Threshold')  # ACWR Risk Threshold
axes[2].set_xlabel('Date')
axes[2].set_ylabel('ACWR')
axes[2].set_title('Acute-to-Chronic Workload Ratio (ACWR) Over Time')
axes[2].legend()
axes[2].grid(True)

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print(physical_df.describe())  # Summary statistics for numerical columns

# Check for missing values
print(physical_df.isnull().sum())

# Look at the unique values in categorical columns
for column in physical_df.select_dtypes(include=['object', 'category']):
  print(f"Unique values in {column}: {physical_df[column].unique()}")

# Create a 1x3 subplot layout
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Correlation Heatmap
correlation_matrix = physical_df.select_dtypes(include=['number']).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", ax=axes[0])
axes[0].set_title('Correlation Matrix')

# Box Plot for a selected numerical feature (e.g., first column)
num_cols = physical_df.select_dtypes(include=['number']).columns
sns.boxplot(x=physical_df[num_cols[0]], ax=axes[1])
axes[1].set_title(f'Box Plot of {num_cols[0]}')

# Histogram for the same numerical feature
axes[2].hist(physical_df[num_cols[0]], bins=20, edgecolor='black')
axes[2].set_title(f'Histogram of {num_cols[0]}')
axes[2].set_xlabel(num_cols[0])
axes[2].set_ylabel('Frequency')

# Adjust layout and show the plot
plt.tight_layout()
plt.show()

variable_df = physical_df.pivot_table(index='ï»¿testDate',
                                     columns='movement',
                                     values='benchmarkPct',
                                     aggfunc='mean')
variable_df= variable_df[['agility', 'jump', 'sprint']]
std_physical = variable_df[['agility', 'jump', 'sprint']].std()

w1, w2, w3 = std_physical['agility'] / std_physical.sum(), std_physical['jump'] / std_physical.sum(), std_physical['sprint'] / std_physical.sum()
variable_df['phys_score'] = ( w1 * variable_df['agility'].fillna(0) +
                                          w2 * variable_df['jump'].fillna(0) +
                                          w3 * variable_df['sprint'].fillna(0))

variable_df.head()

